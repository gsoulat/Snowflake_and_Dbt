# Pipeline NYC Taxi : Snowflake + DBT
## Guide Complet de Transformation de DonnÃ©es Volumineuses

---

## ğŸ“‹ Vue d'Ensemble du Projet

### ğŸ¯ Objectif
CrÃ©er un pipeline de donnÃ©es robuste pour analyser **200+ GB** de donnÃ©es rÃ©elles de taxis NYC avec Snowflake et DBT.

### ğŸ“Š Dataset : NYC Taxi Trip Data
- **Volume** : 3+ milliards de trajets depuis 2009
- **Taille** : 200+ GB compressÃ©s, 750+ GB dÃ©compressÃ©s
- **Source** : [NYC Taxi & Limousine Commission](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
- **Format** : Fichiers Parquet mensuels

### ğŸ—ï¸ Architecture
```
RAW DATA â†’ STAGING â†’ INTERMEDIATE â†’ MARTS
   â†“          â†“           â†“          â†“
Parquet â†’ Nettoyage â†’ Calculs â†’ Analyses
```

---

## ğŸš€ Ã‰tapes de Mise en Å’uvre

### 1. **PrÃ©paration** (30 min)
```bash
# CrÃ©er compte Snowflake (essai gratuit)
# Installer DBT
pip install dbt-snowflake

# VÃ©rifier installation
dbt --version
```

### 2. **Configuration Snowflake** (15 min)
```sql
-- Configuration initiale
USE ROLE ACCOUNTADMIN;

CREATE WAREHOUSE NYC_TAXI_WH WITH
  WAREHOUSE_SIZE = 'LARGE'
  AUTO_SUSPEND = 60;

CREATE DATABASE NYC_TAXI_DB;
CREATE SCHEMA NYC_TAXI_DB.RAW_DATA;
CREATE SCHEMA NYC_TAXI_DB.STAGING; 
CREATE SCHEMA NYC_TAXI_DB.MARTS;
```

### 3. **TÃ©lÃ©chargement des DonnÃ©es** (2-4h)
```python
# Script de tÃ©lÃ©chargement automatisÃ©
import requests
from tqdm import tqdm

def download_nyc_taxi_data(years=[2023, 2024]):
    base_url = "https://d37ci6vzurychx.cloudfront.net/trip-data/"
    # TÃ©lÃ©charge ~50GB pour 2 annÃ©es
```

### 4. **Chargement dans Snowflake** (30 min)
```sql
-- CrÃ©er table de destination
CREATE TABLE yellow_taxi_trips_raw (
    vendorid INTEGER,
    tpep_pickup_datetime TIMESTAMP,
    tpep_dropoff_datetime TIMESTAMP,
    -- ... autres colonnes
);

-- Charger via stage
COPY INTO yellow_taxi_trips_raw 
FROM @nyc_taxi_stage
FILE_FORMAT = (TYPE = 'PARQUET');
```

---

## ğŸ”„ Pipeline de Transformation DBT

### ğŸ“ Structure du Projet
```
models/
â”œâ”€â”€ staging/
â”‚   â””â”€â”€ stg_yellow_taxi_trips.sql      # Nettoyage
â”œâ”€â”€ intermediate/
â”‚   â””â”€â”€ int_trip_metrics.sql           # Calculs
â”œâ”€â”€ marts/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ fact_trips.sql             # Table de faits
â”‚   â””â”€â”€ analytics/
â”‚       â”œâ”€â”€ daily_trip_summary.sql     # RÃ©sumÃ©s quotidiens
â”‚       â”œâ”€â”€ hourly_demand_patterns.sql # Patterns horaires
â”‚       â”œâ”€â”€ location_analysis.sql      # Analyse gÃ©ographique
â”‚       â””â”€â”€ revenue_analysis.sql       # Analyse financiÃ¨re
```

---

## ğŸ§¹ Ã‰TAPE 1 : Staging - Nettoyage des DonnÃ©es

### `staging/stg_yellow_taxi_trips.sql`

#### ğŸš¨ ProblÃ¨mes Ã  Corriger

| **Colonne** | **ProblÃ¨me** | **Solution** |
|-------------|--------------|--------------|
| `passenger_count` | 0, NULL, >6 | NULL/0 â†’ 1, >6 â†’ 6 |
| `trip_distance` | NÃ©gatif, >100 miles | <0 â†’ 0, >100 â†’ NULL |
| `pickup/dropoff_datetime` | pickup > dropoff | Exclure le trajet |
| `fare_amount, tip_amount` | Valeurs nÃ©gatives | <0 â†’ 0 |
| `PULocationID, DOLocationID` | NULL | Exclure le trajet |

#### â• Colonnes DÃ©rivÃ©es AjoutÃ©es
```sql
-- Dimensions temporelles
pickup_date = DATE(tpep_pickup_datetime)
pickup_hour = EXTRACT(HOUR FROM tpep_pickup_datetime)
pickup_day_of_week = EXTRACT(DOW FROM tpep_pickup_datetime)
pickup_month = EXTRACT(MONTH FROM tpep_pickup_datetime)
pickup_year = EXTRACT(YEAR FROM tpep_pickup_datetime)

-- MÃ©triques calculÃ©es
trip_duration_minutes = DATEDIFF(MINUTE, pickup, dropoff)
avg_speed_mph = (trip_distance / trip_duration_minutes) * 60
```

#### ğŸ” Filtres de QualitÃ©
```sql
WHERE tpep_pickup_datetime IS NOT NULL
  AND tpep_dropoff_datetime IS NOT NULL
  AND tpep_pickup_datetime < tpep_dropoff_datetime
  AND tpep_pickup_datetime >= '2009-01-01'
  AND pulocationid IS NOT NULL
  AND dolocationid IS NOT NULL
```

---

## âš™ï¸ Ã‰TAPE 2 : Intermediate - Calculs et CatÃ©gorisations

### `intermediate/int_trip_metrics.sql`

#### ğŸ“Š CatÃ©gories MÃ©tier CrÃ©Ã©es

**Distance du Trajet**
```sql
distance_category = CASE 
    WHEN trip_distance <= 1 THEN 'Court (â‰¤1 mile)'
    WHEN trip_distance <= 3 THEN 'Moyen (1-3 miles)'
    WHEN trip_distance <= 10 THEN 'Long (3-10 miles)'
    ELSE 'TrÃ¨s long (>10 miles)'
END
```

**DurÃ©e du Trajet**
```sql
duration_category = CASE 
    WHEN trip_duration_minutes <= 10 THEN 'Rapide (â‰¤10 min)'
    WHEN trip_duration_minutes <= 30 THEN 'Normal (10-30 min)'
    WHEN trip_duration_minutes <= 60 THEN 'Long (30-60 min)'
    ELSE 'TrÃ¨s long (>60 min)'
END
```

**PÃ©riode de la JournÃ©e**
```sql
time_period = CASE 
    WHEN pickup_hour BETWEEN 6 AND 9 THEN 'Rush Matinal'
    WHEN pickup_hour BETWEEN 10 AND 15 THEN 'JournÃ©e'
    WHEN pickup_hour BETWEEN 16 AND 19 THEN 'Rush Soir'
    WHEN pickup_hour BETWEEN 20 AND 23 THEN 'SoirÃ©e'
    ELSE 'Nuit'
END
```

**Type de Jour**
```sql
day_name = CASE pickup_day_of_week
    WHEN 0 THEN 'Dimanche'
    WHEN 1 THEN 'Lundi'
    -- ... etc
END

is_weekend = CASE 
    WHEN pickup_day_of_week IN (0, 6) THEN 'Weekend'
    ELSE 'Semaine'
END
```

**MÃ©trique FinanciÃ¨re**
```sql
tip_rate_percent = CASE 
    WHEN fare_amount > 0 
    THEN (tip_amount / fare_amount) * 100
    ELSE 0
END
```

---

## ğŸ¯ Ã‰TAPE 3 : Marts Core - Table de Faits Finale

### `marts/core/fact_trips.sql`

#### ğŸ—ï¸ Structure de la Table de Faits

**ğŸ”‘ ClÃ©s et Identifiants**
```sql
trip_key = generate_surrogate_key([pickup_datetime, pulocationid, dolocationid, fare_amount])
```

**ğŸ“… Dimensions Temporelles**
```sql
pickup_date, pickup_hour, pickup_day_of_week, day_name, 
is_weekend, time_period, pickup_month, pickup_year
```

**ğŸ—ºï¸ Dimensions GÃ©ographiques**
```sql
pulocationid, dolocationid
```

**ğŸš— Dimensions de Voyage**
```sql
passenger_count, distance_category, duration_category, 
payment_type, vendorid
```

**ğŸ“Š MÃ©triques de Performance**
```sql
-- Distance et Temps
trip_distance, trip_duration_minutes, avg_speed_mph

-- Financier
fare_amount, tip_amount, tip_rate_percent, extra, mta_tax,
tolls_amount, improvement_surcharge, congestion_surcharge, 
airport_fee, total_amount
```

**ğŸ”§ Optimisations Techniques**
```sql
-- Index pour performance
indexes=[
  {'columns': ['pickup_date'], 'type': 'btree'},
  {'columns': ['pulocationid', 'dolocationid'], 'type': 'btree'}
]
```

---

## ğŸ“ˆ Ã‰TAPE 4 : Marts Analytics - Tables d'Analyse

### 1. `daily_trip_summary.sql` - RÃ©sumÃ© Quotidien

#### ğŸ“Š MÃ©triques de Volume
```sql
total_trips = COUNT(*)
unique_pickup_zones = COUNT(DISTINCT pulocationid)
unique_dropoff_zones = COUNT(DISTINCT dolocationid)
```

#### ğŸš€ MÃ©triques de Performance
```sql
avg_trip_distance = AVG(trip_distance)
median_trip_distance = MEDIAN(trip_distance)
avg_trip_duration = AVG(trip_duration_minutes)
avg_speed = AVG(avg_speed_mph)
```

#### ğŸ’° MÃ©triques FinanciÃ¨res
```sql
total_revenue = SUM(total_amount)
avg_trip_value = AVG(total_amount)
total_tips = SUM(tip_amount)
avg_tip_rate = AVG(tip_rate_percent)
```

#### ğŸ• RÃ©partition Temporelle
```sql
morning_rush_trips = SUM(CASE WHEN time_period = 'Rush Matinal' THEN 1 ELSE 0 END)
evening_rush_trips = SUM(CASE WHEN time_period = 'Rush Soir' THEN 1 ELSE 0 END)
night_trips = SUM(CASE WHEN time_period = 'Nuit' THEN 1 ELSE 0 END)
```

### 2. `hourly_demand_patterns.sql` - Patterns Horaires

#### ğŸ” Analyses par Heure
```sql
-- Volume et revenus par heure de la journÃ©e
trips_per_hour, revenue_per_hour, avg_speed_by_hour

-- Identification des pics de demande
peak_hours, demand_intensity, surge_indicators

-- Patterns weekend vs semaine
weekend_vs_weekday_patterns
```

### 3. `location_analysis.sql` - Analyse GÃ©ographique

#### ğŸ—ºï¸ MÃ©triques par Zone
```sql
-- PopularitÃ© des zones
pickup_volume_by_zone = COUNT(*) GROUP BY pulocationid
dropoff_volume_by_zone = COUNT(*) GROUP BY dolocationid

-- RentabilitÃ© par zone
avg_fare_by_pickup_zone = AVG(fare_amount) GROUP BY pulocationid
most_profitable_routes = TOP routes by total_revenue

-- Patterns spÃ©ciaux
airport_traffic_patterns = WHERE airport_fee > 0
manhattan_vs_outer_boroughs = comparaison volumes/revenus
```

### 4. `revenue_analysis.sql` - Analyse FinanciÃ¨re

#### ğŸ’³ Analyse des Pourboires
```sql
-- Par type de paiement
tip_analysis_by_payment_type = AVG(tip_rate_percent) GROUP BY payment_type

-- Par distance et durÃ©e
tip_correlation_with_distance = correlation(tip_rate, distance_category)
tip_patterns_by_time = AVG(tip_rate) GROUP BY time_period
```

#### ğŸ“ˆ Optimisation Tarifaire
```sql
-- OpportunitÃ©s d'optimisation
fare_optimization_opportunities = zones sous-facturÃ©es
revenue_by_distance_category = rentabilitÃ© par type de trajet
price_elasticity_indicators = sensibilitÃ© prix/demande
```

---

## ğŸ§ª Validation et Tests

### ğŸ” Tests de QualitÃ© des DonnÃ©es
```sql
-- Tests automatiques DBT
tests:
  - unique: trip_key
  - not_null: [pickup_date, total_amount]
  - accepted_range: 
      min_value: 0
      max_value: 100 (pour trip_distance)
```

### âœ… Tests MÃ©tier
```sql
-- DurÃ©es de trajet positives
assert_positive_trip_duration.sql

-- CohÃ©rence des montants
assert_total_amount_consistency.sql

-- Zones valides uniquement
assert_valid_location_ids.sql
```

---

## ğŸš€ DÃ©ploiement et ExÃ©cution

### ğŸ“‹ Commandes Essentielles
```bash
# Validation de la configuration
dbt debug

# ExÃ©cution du pipeline complet
dbt deps        # Installer les packages
dbt run         # ExÃ©cuter les transformations
dbt test        # Valider la qualitÃ©
dbt docs generate  # GÃ©nÃ©rer la documentation

# ExÃ©cution ciblÃ©e
dbt run --select stg_yellow_taxi_trips+
dbt test --select fact_trips
```

### ğŸ”„ Workflow de DÃ©veloppement
```bash
# DÃ©veloppement itÃ©ratif
dbt run --select nouveau_modele
dbt test --select nouveau_modele

# Validation complÃ¨te avant production
dbt build  # run + test en une commande
```

---

## ğŸ“Š RÃ©sultats Attendus

### ğŸ¯ Tables Finales CrÃ©Ã©es

| **Table** | **Type** | **VolumÃ©trie** | **Usage** |
|-----------|----------|----------------|-----------|
| `fact_trips` | Table de faits | 3+ milliards de lignes | Analyses dÃ©taillÃ©es |
| `daily_trip_summary` | AgrÃ©gat | ~5,000 lignes | Reporting quotidien |
| `hourly_demand_patterns` | AgrÃ©gat | ~200,000 lignes | Analyse patterns |
| `location_analysis` | AgrÃ©gat | ~500 lignes | GÃ©o-analytics |
| `revenue_analysis` | AgrÃ©gat | ~10,000 lignes | Analyses financiÃ¨res |

### ğŸ“ˆ KPIs Disponibles

**OpÃ©rationnels**
- Volume de trajets par jour/heure/zone
- Vitesses moyennes par zone et pÃ©riode
- Patterns de demande saisonniers

**Financiers**
- Chiffre d'affaires total et par segment
- Ã‰volution des pourboires par type de paiement
- RentabilitÃ© par zone gÃ©ographique

**Satisfaction Client**
- DurÃ©es moyennes de trajet
- Taux de pourboire comme proxy satisfaction
- Zones prÃ©fÃ©rÃ©es et Ã©vitÃ©es

---

## ğŸ”§ Optimisations et Bonnes Pratiques

### âš¡ Performance
```sql
-- Partitionnement par date
CLUSTER BY (pickup_date)

-- MatÃ©rialisation adaptÃ©e
models:
  staging: materialized='view'      # Transformations lÃ©gÃ¨res
  marts: materialized='table'       # Tables frÃ©quemment requÃªtÃ©es
```

### ğŸ”’ SÃ©curitÃ©
```yaml
# Variables d'environnement
password: "{{ env_var('DBT_SNOWFLAKE_PASSWORD') }}"

# Permissions minimales par environnement
dev: lecture/Ã©criture schÃ©mas staging
prod: lecture seule schÃ©mas marts
```

### ğŸ“š Documentation
```yaml
# Description de chaque modÃ¨le
models:
  - name: fact_trips
    description: "Table de faits des trajets avec mÃ©triques calculÃ©es"
    columns:
      - name: trip_key
        description: "ClÃ© unique gÃ©nÃ©rÃ©e pour chaque trajet"
```

---

## ğŸ¯ Cas d'Usage AvancÃ©s

### ğŸ¤– Machine Learning
- **PrÃ©diction de demande** : Forecast des trajets par zone/heure
- **DÃ©tection d'anomalies** : Trajets suspects ou erreurs de capteur
- **Optimisation d'itinÃ©raires** : Recommandations basÃ©es sur patterns historiques

### ğŸ“Š Business Intelligence
- **Dashboards opÃ©rationnels** : Monitoring temps rÃ©el de la flotte
- **Analyses stratÃ©giques** : Expansion gÃ©ographique, optimisation tarifaire
- **Rapports rÃ©glementaires** : ConformitÃ© avec les autoritÃ©s de transport

### ğŸ” Analytics AvancÃ©s
- **Segmentation clients** : Profils de voyage par type d'utilisateur
- **Analyse de cohortes** : Ã‰volution des patterns de mobilitÃ©
- **Impact Ã©vÃ©nementiel** : Influence mÃ©tÃ©o, Ã©vÃ©nements sur la demande

---

## â±ï¸ Timeline du Projet

| **Phase** | **DurÃ©e** | **ActivitÃ©s** |
|-----------|-----------|---------------|
| **Setup** | 1h | Comptes, installations, configuration |
| **Data Loading** | 3-4h | TÃ©lÃ©chargement et chargement donnÃ©es |
| **DBT Development** | 4-6h | DÃ©veloppement modÃ¨les et tests |
| **Validation** | 1-2h | Tests, documentation, optimisation |
| **DÃ©ploiement** | 1h | Production, monitoring |
| **TOTAL** | **10-14h** | **Projet complet fonctionnel** |

---

## ğŸ‰ Conclusion

Ce pipeline vous fournit :

âœ… **Infrastructure robuste** avec Snowflake  
âœ… **Transformations documentÃ©es** avec DBT  
âœ… **DonnÃ©es prÃªtes pour l'analyse** avec 750+ GB nettoyÃ©s  
âœ… **Tests automatisÃ©s** pour la qualitÃ©  
âœ… **Documentation auto-gÃ©nÃ©rÃ©e** pour la maintenance  

**ğŸš€ Prochaines Ã©tapes :**
1. DÃ©marrer avec quelques mois de donnÃ©es pour validation
2. Ã‰tendre progressivement le dataset
3. Ajouter des analyses spÃ©cifiques mÃ©tier
4. IntÃ©grer des outils de visualisation
5. Explorer le machine learning sur les donnÃ©es transformÃ©es

**ğŸ’¡ Ce projet vous donne une base solide pour devenir expert en ingÃ©nierie de donnÃ©es moderne !**